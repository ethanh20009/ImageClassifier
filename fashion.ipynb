{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "trainDataset = torchvision.datasets.FashionMNIST(\"fashionDataset\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "x,y = trainDataset[0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RisNN(\n",
       "  (resBlocks): Sequential(\n",
       "    (0): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): resBlock(\n",
       "      (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 3, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(3, 9, kernel_size=5, padding=2)\n",
    "        self.conv3 = torch.nn.Conv2d(9, 20, kernel_size=5, padding=2)\n",
    "        self.conv4 = torch.nn.Conv2d(20, 25, kernel_size=5, padding=2)\n",
    "        self.fc1 = torch.nn.Linear(225, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, 10)\n",
    "        self.drop = torch.nn.Dropout(0.20)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2) #Halfs image\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            self.pool,\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(self.conv1(x))\n",
    "        x = self.conv(self.conv2(x))\n",
    "        x = self.conv(self.conv3(x))\n",
    "        x = torch.nn.functional.relu(self.conv4(x))\n",
    "        x = x.reshape((-1, 225))\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class RisNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resBlocks = torch.nn.Sequential(\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1),\n",
    "            resBlock(1,1)\n",
    "        )\n",
    "        self.fc1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 10)\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self,x): \n",
    "        x = self.resBlocks(x)\n",
    "        x = x.reshape((-1, 784))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "class resBlock(torch.nn.Module):\n",
    "    def __init__(self, inChannels, outChannels):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(inChannels, outChannels, kernel_size=5, padding=2)\n",
    "        self.conv2 = torch.nn.Conv2d(outChannels, outChannels, kernel_size=5, padding=2)\n",
    "        self.bn = torch.nn.BatchNorm2d(outChannels, track_running_stats=False)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn(y)\n",
    "        y = self.relu(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "    \n",
    "test = torch.randn(size=(100, 1, 28, 28),device=device)\n",
    "model = RisNN()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batchSize = 600\n",
    "dataloader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25760\\964358739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0myBatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myBatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0my_this\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_this\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myBatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25760\\216688848.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresBlocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25760\\216688848.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    totalCost = 0\n",
    "  \n",
    "    for xBatch, yBatch in dataloader:\n",
    "        optimiser.zero_grad()\n",
    "        xBatch = xBatch.to(device)\n",
    "        yBatch = yBatch.to(device)\n",
    "        \n",
    "        y_this = model(xBatch)\n",
    "        cost = loss(y_this, yBatch)\n",
    "        cost.backward()\n",
    "        totalCost += cost.item()\n",
    "\n",
    "        optimiser.step()\n",
    "    ##Plot acc\n",
    "    ySet = trainDataset.targets.to(device)\n",
    "    xSet = trainDataset.data.reshape(-1, 1, 28, 28)\n",
    "    xSet = xSet.to(device).to(torch.float)\n",
    "    modelOutput = model(xSet)\n",
    "\n",
    "    indexOut = torch.argmax(modelOutput, dim=1)\n",
    "    matches = indexOut == ySet\n",
    "    accuracy.append(matches.sum().item()*100/len(ySet))\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(accuracy)\n",
    "    plt.show()\n",
    "    print(i)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    history.append(totalCost/len(trainDataset))\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resModelParams.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ba8e025f40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "testImage, expectedOutput = trainDataset[20]\n",
    "testImage = testImage.reshape(1,28,28)\n",
    "modelOutput = model(testImage.to(device).reshape(-1, 1, 28, 28))\n",
    "index = torch.argmax(modelOutput[0], dim=0).item()\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "plt.title( labels[index])\n",
    "plt.xlabel(\"True: \"+labels[expectedOutput])\n",
    "plt.imshow(testImage[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.32666666666667\n"
     ]
    }
   ],
   "source": [
    "##Test Entire ds for accuracy\n",
    "model.eval()\n",
    "ySet = trainDataset.targets.to(device)\n",
    "xSet = trainDataset.data.reshape(-1, 1, 28, 28)\n",
    "xSet = xSet.to(device).to(torch.float)\n",
    "modelOutput = model(xSet)\n",
    "\n",
    "indexOut = torch.argmax(modelOutput, dim=1)\n",
    "matches = indexOut == ySet\n",
    "print(matches.sum().item()*100/len(ySet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79538\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"resModelParams.pt\", map_location=device))\n",
    "\n",
    "#get number of parameters in model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
